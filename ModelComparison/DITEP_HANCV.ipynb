{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os,re\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import make_scorer,f1_score,recall_score,fbeta_score,\\\n",
    "precision_recall_fscore_support,accuracy_score,precision_score\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "#from lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=10):\n",
    "        '''\n",
    "        Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "        '''\n",
    "        def schedule(epoch):\n",
    "            return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "        \n",
    "        return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "class AttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self, context_vector_length=100, **kwargs):\n",
    "        \"\"\"\n",
    "        An implementation of a attention layer. This layer\n",
    "        accepts a 3d Tensor (batch_size, time_steps, input_dim) and\n",
    "        applies a single layer attention mechanism in the time\n",
    "        direction (the second axis).\n",
    "        :param context_vector_lenght: (int) The size of the hidden context vector.\n",
    "            If set to 1 this layer reduces to a standard attention layer.\n",
    "        :param kwargs: Any argument that the baseclass Layer accepts.\n",
    "        \"\"\"\n",
    "        self.context_vector_length = context_vector_length\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[2]\n",
    "\n",
    "        # Add a weights layer for the\n",
    "        self.W = self.add_weight(\n",
    "            name='W', shape=(dim, self.context_vector_length),\n",
    "            initializer=keras.initializers.get('uniform'),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.u = self.add_weight(\n",
    "            name='context_vector', shape=(self.context_vector_length, 1),\n",
    "            initializer=keras.initializers.get('uniform'),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def _get_attention_weights(self, X):\n",
    "        \"\"\"\n",
    "        Computes the attention weights for each timestep in X\n",
    "        :param X: 3d-tensor (batch_size, time_steps, input_dim)\n",
    "        :return: 2d-tensor (batch_size, time_steps) of attention weights\n",
    "        \"\"\"\n",
    "        # Compute a time-wise stimulus, i.e. a stimulus for each\n",
    "        # time step. For this first compute a hidden layer of\n",
    "        # dimension self.context_vector_length and take the\n",
    "        # similarity of this layer with self.u as the stimulus\n",
    "        u_tw = K.tanh(K.dot(X, self.W))\n",
    "        tw_stimulus = K.dot(u_tw, self.u)\n",
    "\n",
    "        # Remove the last axis an apply softmax to the stimulus to\n",
    "        # get a probability.\n",
    "        tw_stimulus = K.reshape(tw_stimulus, (-1, tw_stimulus.shape[1]))\n",
    "        att_weights = K.softmax(tw_stimulus)\n",
    "\n",
    "        return att_weights\n",
    "\n",
    "    def call(self, X):\n",
    "        att_weights = self._get_attention_weights(X)\n",
    "\n",
    "        # Reshape the attention weights to match the dimensions of X\n",
    "        att_weights = K.reshape(att_weights, (-1, att_weights.shape[1], 1))\n",
    "        att_weights = K.repeat_elements(att_weights, X.shape[-1], -1)\n",
    "\n",
    "        # Multiply each input by its attention weights\n",
    "        weighted_input = keras.layers.Multiply()([X, att_weights])\n",
    "\n",
    "        # Sum in the direction of the time-axis.\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'context_vector_length': self.context_vector_length\n",
    "        }\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import (\n",
    "    Dense, GRU, TimeDistributed, Input,\n",
    "    Embedding, Bidirectional, Lambda\n",
    ")\n",
    "from keras.models import Model\n",
    "#from keras_han.layers import AttentionLayer\n",
    "\n",
    "\n",
    "class HAN(Model):\n",
    "    def __init__(\n",
    "            self, max_words, max_sentences, output_size,\n",
    "            embedding_matrix, word_encoding_dim,\n",
    "            sentence_encoding_dim, \n",
    "            kernel_initializer,\n",
    "                      bias_initializer,\n",
    "                      kernel_regularizer,\n",
    "                      recurrent_regularizer,\n",
    "                      bias_regularizer,\n",
    "                      activity_regularizer,\n",
    "                      kernel_constraint,\n",
    "                      recurrent_constraint,\n",
    "                      bias_constraint,\n",
    "                      dropout,\n",
    "                      recurrent_dropout,\n",
    "                      kernel_initializer2,\n",
    "                      bias_initializer2,\n",
    "                      kernel_regularizer2,\n",
    "                      recurrent_regularizer2,\n",
    "                      bias_regularizer2,\n",
    "                      activity_regularizer2,\n",
    "                      kernel_constraint2,\n",
    "                      recurrent_constraint2,\n",
    "                      bias_constraint2,\n",
    "                      dropout2,\n",
    "                      recurrent_dropout2,\n",
    "                      inputs=None,\n",
    "            outputs=None, name='han-for-docla'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A Keras implementation of Hierarchical Attention networks\n",
    "        for document classification.\n",
    "        :param max_words: The maximum number of words per sentence\n",
    "        :param max_sentences: The maximum number of sentences\n",
    "        :param output_size: The dimension of the last layer (i.e.\n",
    "            the number of classes you wish to predict)\n",
    "        :param embedding_matrix: The embedding matrix to use for\n",
    "            representing words\n",
    "        :param word_encoding_dim: The dimension of the GRU\n",
    "            layer in the word encoder.\n",
    "        :param sentence_encoding_dim: The dimension of the GRU\n",
    "            layer in the sentence encoder.\n",
    "        \"\"\"\n",
    "        self.max_words = max_words\n",
    "        self.max_sentences = max_sentences\n",
    "        self.output_size = output_size\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.word_encoding_dim = word_encoding_dim\n",
    "        self.sentence_encoding_dim = sentence_encoding_dim\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.recurrent_regularizer = recurrent_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "        self.activity_regularizer = activity_regularizer\n",
    "        self.kernel_constraint = kernel_constraint\n",
    "        self.recurrent_constraint = recurrent_constraint\n",
    "        self.bias_constraint = bias_constraint\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.kernel_initializer2 = kernel_initializer2\n",
    "        self.bias_initializer2 = bias_initializer2\n",
    "        self.kernel_regularizer2 = kernel_regularizer2\n",
    "        self.recurrent_regularizer2 = recurrent_regularizer2\n",
    "        self.bias_regularizer2 = bias_regularizer2\n",
    "        self.activity_regularizer2 = activity_regularizer2\n",
    "        self.kernel_constraint2 = kernel_constraint2\n",
    "        self.recurrent_constraint2 = recurrent_constraint2\n",
    "        self.bias_constraint2 = bias_constraint2\n",
    "        self.dropout2 = dropout2\n",
    "        self.recurrent_dropout2 = recurrent_dropout2\n",
    "        \n",
    "        \n",
    "        in_tensor, out_tensor = self._build_network()\n",
    "\n",
    "        super(HAN, self).__init__(\n",
    "            inputs=in_tensor, outputs=out_tensor, name=name\n",
    "        )\n",
    "\n",
    "    def build_word_encoder(self, max_words, embedding_matrix,encoding_dim=200):\n",
    "        \"\"\"\n",
    "        Build the model that embeds and encodes in context the\n",
    "        words used in a sentence. The return model takes a tensor of shape\n",
    "        (batch_size, max_length) that represents a collection of sentences\n",
    "        and returns an encoded representation of these sentences.\n",
    "        :param max_words: (int) The maximum sentence length this model accepts\n",
    "        :param embedding_matrix: (2d array-like) A matrix with the i-th row\n",
    "            representing the embedding of the word represented by index i.\n",
    "        :param encoding_dim: (int, should be even) The dimension of the\n",
    "            bidirectional encoding layer. Half of the nodes are used in the\n",
    "            forward direction and half in the backward direction.\n",
    "        :return: Instance of keras.Model\n",
    "        \"\"\"\n",
    "        assert encoding_dim % 2 == 0, \"Embedding dimension should be even\"\n",
    "\n",
    "        vocabulary_size = embedding_matrix.shape[0]\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        embedding_layer = Embedding(\n",
    "            vocabulary_size, embedding_dim,\n",
    "            weights=[embedding_matrix], input_length=max_words,\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "        sentence_input = Input(shape=(max_words,), dtype='int32')\n",
    "        embedded_sentences = embedding_layer(sentence_input)\n",
    "        encoded_sentences = Bidirectional(\n",
    "            GRU(int(encoding_dim / 2), return_sequences=True,\n",
    "                      kernel_initializer = self.kernel_initializer,\n",
    "                      bias_initializer = self.bias_initializer,\n",
    "                      kernel_regularizer = self.kernel_regularizer,\n",
    "                      recurrent_regularizer = self.recurrent_regularizer,\n",
    "                      bias_regularizer = self.bias_regularizer,\n",
    "                      activity_regularizer = self.activity_regularizer,\n",
    "                      kernel_constraint = self.kernel_constraint,\n",
    "                      recurrent_constraint = self.recurrent_constraint,\n",
    "                      bias_constraint = self.bias_constraint,\n",
    "                      dropout = self.dropout,\n",
    "                      recurrent_dropout = self.recurrent_dropout\n",
    "                )\n",
    "        )(embedded_sentences)\n",
    "\n",
    "        return Model(\n",
    "            inputs=[sentence_input], outputs=[encoded_sentences], name='word_encoder'\n",
    "        )\n",
    "\n",
    "    def build_sentence_encoder(self, max_sentences, summary_dim, encoding_dim):\n",
    "        \"\"\"\n",
    "        Build the encoder that encodes the vector representation of\n",
    "        sentences in their context.\n",
    "        :param max_sentences: The maximum number of sentences that can be\n",
    "            passed. Use zero-padding to supply shorter sentences.\n",
    "        :param summary_dim: (int) The dimension of the vectors that summarizes\n",
    "            sentences. Should be equal to the encoding_dim of the word\n",
    "            encoder.\n",
    "        :param encoding_dim: (int, even) The dimension of the vector that\n",
    "            summarizes sentences in context. Half is used in forward direction,\n",
    "            half in backward direction.\n",
    "        :return: Instance of keras.Model\n",
    "        \"\"\"\n",
    "        assert encoding_dim % 2 == 0, \"Embedding dimension should be even\"\n",
    "\n",
    "        text_input = Input(shape=(max_sentences, summary_dim))\n",
    "        encoded_sentences = Bidirectional(\n",
    "            GRU(int(encoding_dim / 2), return_sequences=True,\n",
    "                kernel_initializer = self.kernel_initializer2,\n",
    "                      bias_initializer = self.bias_initializer2,\n",
    "                      kernel_regularizer = self.kernel_regularizer2,\n",
    "                      recurrent_regularizer = self.recurrent_regularizer2,\n",
    "                      bias_regularizer = self.bias_regularizer2,\n",
    "                      activity_regularizer = self.activity_regularizer2,\n",
    "                      kernel_constraint = self.kernel_constraint2,\n",
    "                      recurrent_constraint = self.recurrent_constraint2,\n",
    "                      bias_constraint = self.bias_constraint2,\n",
    "                      dropout = self.dropout2,\n",
    "                      recurrent_dropout = self.recurrent_dropout2)\n",
    "        )(text_input)\n",
    "        return Model(\n",
    "            inputs=[text_input], outputs=[encoded_sentences], name='sentence_encoder'\n",
    "        )\n",
    "\n",
    "    def _build_network(self):\n",
    "        \"\"\"\n",
    "        Build the graph that represents this network\n",
    "        :return: in_tensor, out_tensor, Tensors representing the input and output\n",
    "            of this network.\n",
    "        \"\"\"\n",
    "        in_tensor = Input(shape=(self.max_sentences, self.max_words))\n",
    "\n",
    "        word_encoder = self.build_word_encoder(\n",
    "            self.max_words, self.embedding_matrix, self.word_encoding_dim\n",
    "        )\n",
    "\n",
    "        word_rep = TimeDistributed(\n",
    "            word_encoder, name='word_encoder'\n",
    "        )(in_tensor)\n",
    "\n",
    "        # Sentence Rep is a 3d-tensor (batch_size, max_sentences, word_encoding_dim)\n",
    "        sentence_rep = TimeDistributed(\n",
    "            AttentionLayer(), name='word_attention'\n",
    "        )(word_rep)\n",
    "\n",
    "        doc_rep = self.build_sentence_encoder(\n",
    "            self.max_sentences, self.word_encoding_dim, self.sentence_encoding_dim\n",
    "        )(sentence_rep)\n",
    "\n",
    "        # We get the final representation by applying our attention mechanism\n",
    "        # to the encoded sentences\n",
    "        doc_summary = AttentionLayer(name='sentence_attention')(doc_rep)\n",
    "        \n",
    "        out_tensor = Dense(\n",
    "            self.output_size, activation='sigmoid', name='class_prediction' #softmax for categories\n",
    "        )(doc_summary)\n",
    "\n",
    "        return in_tensor, out_tensor\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'max_words': self.max_words,\n",
    "            'max_sentences': self.max_sentences,\n",
    "            'output_size': self.output_size,\n",
    "            'embedding_matrix': self.embedding_matrix,\n",
    "            'word_encoding_dim': self.word_encoding_dim,\n",
    "            'sentence_encoding_dim': self.sentence_encoding_dim,\n",
    "            'base_config': super(HAN, self).get_config()\n",
    "        }\n",
    "\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None):\n",
    "        \"\"\"\n",
    "        Keras' API isn't really extendible at this point\n",
    "        therefore we need to use a bit hacky solution to\n",
    "        be able to correctly reconstruct the HAN model\n",
    "        from a config. This therefore does not reconstruct\n",
    "        a instance of HAN model, but actually a standard\n",
    "        Keras model that behaves exactly the same.\n",
    "        \"\"\"\n",
    "        base_config = config.pop('base_config')\n",
    "\n",
    "        return Model.from_config(\n",
    "            base_config, custom_objects=custom_objects\n",
    "        )\n",
    "\n",
    "    def predict_sentence_attention(self, X):\n",
    "        \"\"\"\n",
    "        For a given set of texts predict the attention\n",
    "        weights for each sentence.\n",
    "        :param X: 3d-tensor, similar to the input for predict\n",
    "        :return: 2d array (num_obs, max_sentences) containing\n",
    "            the attention weights for each sentence\n",
    "        \"\"\"\n",
    "        att_layer = self.get_layer('sentence_attention')\n",
    "        prev_tensor = att_layer.input\n",
    "\n",
    "        # Create a temporary dummy layer to hold the\n",
    "        # attention weights tensor\n",
    "        dummy_layer = Lambda(\n",
    "            lambda x: att_layer._get_attention_weights(x)\n",
    "        )(prev_tensor)\n",
    "\n",
    "        return Model(self.input, dummy_layer).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a logger to provide info on the state of the\n",
    "# script\n",
    "stdout = logging.StreamHandler(sys.stdout)\n",
    "stdout.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "))\n",
    "logger = logging.getLogger('default')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(stdout)\n",
    "\n",
    "MAX_WORDS_PER_SENT = 100 #to TUNE\n",
    "MAX_SENT = 15 #to TUNE\n",
    "max_words = 10000 #to TUNE\n",
    "embedding_dim = 256 #or 128\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Pre-processsing data.\")\n",
    "\n",
    "#This part has to be redone, due to the wrong format of the data coming from Valentin's processing\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger.info(\"Tokenization.\")\n",
    "\n",
    "# Build a Keras Tokenizer that can encode every token\n",
    "word_tokenizer = Tokenizer(num_words=max_words)\n",
    "word_tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "# Construct the input matrix. This should be a nd-array of\n",
    "# shape (n_samples, MAX_SENT, MAX_WORDS_PER_SENT).\n",
    "# We zero-pad this matrix (this does not influence\n",
    "# any predictions due to the attention mechanism.\n",
    "X = np.zeros((len(reviews), MAX_SENT, MAX_WORDS_PER_SENT), dtype='int32')\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    sentences = sent_tokenize(review)\n",
    "    tokenized_sentences = word_tokenizer.texts_to_sequences(\n",
    "        sentences\n",
    "    )\n",
    "    tokenized_sentences = pad_sequences(\n",
    "        tokenized_sentences, maxlen=MAX_WORDS_PER_SENT\n",
    "    )\n",
    "\n",
    "    pad_size = MAX_SENT - tokenized_sentences.shape[0]\n",
    "\n",
    "    if pad_size < 0:\n",
    "        tokenized_sentences = tokenized_sentences[0:MAX_SENT]\n",
    "    else:\n",
    "        tokenized_sentences = np.pad(\n",
    "            tokenized_sentences, ((0,pad_size),(0,0)),\n",
    "            mode='constant', constant_values=0\n",
    "        )\n",
    "\n",
    "    # Store this observation as the i-th observation in\n",
    "    # the data matrix\n",
    "    X[i] = tokenized_sentences[None, ...]\n",
    "\n",
    "# Transform the labels into a format Keras can handle\n",
    "y = np.asarray(target)#to_categorical(labels)\n",
    "\n",
    "# We make a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"Creating embedding matrix using pre-trained w2v vectors.\"\n",
    ")\n",
    "\n",
    "# Now, we need to build the embedding matrix. For this we use\n",
    "# a pretrained (on the wikipedia corpus) 100-dimensional GloVe\n",
    "# model.\n",
    "\n",
    "# Load the embeddings from a file\n",
    "embeddings = {}\n",
    "embedding_name = 'w2v_reports_256.vec'\n",
    "with open(embedding_name, encoding='utf-8') as file:\n",
    "    dummy = file.readline()\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        embeddings[word] = coefs\n",
    "\n",
    "# Initialize a matrix to hold the word embeddings\n",
    "embedding_matrix = np.random.random(\n",
    "    (len(word_tokenizer.word_index) + 1, embedding_dim)\n",
    ")\n",
    "\n",
    "# Let the padded indices map to zero-vectors. This will\n",
    "# prevent the padding from influencing the results\n",
    "embedding_matrix[0] = 0\n",
    "\n",
    "# Loop though all the words in the word_index and where possible\n",
    "# replace the random initalization with the GloVe vector.\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(optimizer,\n",
    "                 kernel_initializer,\n",
    "                     bias_initializer,\n",
    "                      kernel_regularizer,\n",
    "                      recurrent_regularizer,\n",
    "                      bias_regularizer,\n",
    "                      activity_regularizer,\n",
    "                      kernel_constraint,\n",
    "                      recurrent_constraint,\n",
    "                      bias_constraint,\n",
    "                      dropout,\n",
    "                      recurrent_dropout,\n",
    "                      kernel_initializer2,\n",
    "                     bias_initializer2,\n",
    "                      kernel_regularizer2,\n",
    "                      recurrent_regularizer2,\n",
    "                      bias_regularizer2,\n",
    "                      activity_regularizer2,\n",
    "                      kernel_constraint2,\n",
    "                      recurrent_constraint2,\n",
    "                      bias_constraint2,\n",
    "                      dropout2,\n",
    "                      recurrent_dropout2,\n",
    "                      word_encoding_dim,\n",
    "                      sentence_encoding_dim\n",
    "                      ):\n",
    "\n",
    "\n",
    "    han_model = HAN(\n",
    "        MAX_WORDS_PER_SENT, MAX_SENT, 1, embedding_matrix, #2 is output size\n",
    "        word_encoding_dim, sentence_encoding_dim, #number of units fir the 2 GRUs\n",
    "                      kernel_initializer,\n",
    "                      bias_initializer,\n",
    "                      kernel_regularizer,\n",
    "                      recurrent_regularizer,\n",
    "                      bias_regularizer,\n",
    "                      activity_regularizer,\n",
    "                      kernel_constraint,\n",
    "                      recurrent_constraint,\n",
    "                      bias_constraint,\n",
    "                      dropout,\n",
    "                      recurrent_dropout,\n",
    "                      kernel_initializer2,\n",
    "                      bias_initializer2,\n",
    "                      kernel_regularizer2,\n",
    "                      recurrent_regularizer2,\n",
    "                      bias_regularizer2,\n",
    "                      activity_regularizer2,\n",
    "                      kernel_constraint2,\n",
    "                      recurrent_constraint2,\n",
    "                      bias_constraint2,\n",
    "                      dropout2,\n",
    "                      recurrent_dropout2\n",
    "    )\n",
    "    \n",
    "    han_model.summary()\n",
    "    \n",
    "    han_model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "    \n",
    "    #attentionWeights = han_model.predict_sentence_attention(X)\n",
    "\n",
    "    #np.savetxt(\"attention.csv\", attentionWeights, delimiter=\",\", fmt='%s', header=None)\n",
    "    \n",
    "    return han_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single cross validation (i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('HAN-gridoutput.txt','w')\n",
    "for i in range(1): #No need for a double cross validation\n",
    "    \n",
    "    #Possible parameter ranges\n",
    "    batch_size = [16,32,64,128]\n",
    "    epochs = [50]\n",
    "    word_encoding_dim = [64,100,128,200,256]\n",
    "    sentence_encoding_dim = [16,32,64,100,128]\n",
    "    \n",
    "    #GRU params\n",
    "    #activation = ['hard_sigmoid','softmax','elu','selu','softplus','softsign','relu',\n",
    "    #                      'tanh','sigmoid','exponential','linear','PReLU','LeakyReLu']\n",
    "    #recurrent_activation=['hard_sigmoid','softmax','elu','selu','softplus','softsign','relu',\n",
    "    #                      'tanh','sigmoid','exponential','linear','PReLU','LeakyReLu']\n",
    "    kernel_initializer=['glorot_normal','glorot_uniform','TruncatedNormal','VarianceScaling'] #cause it's a tanh\n",
    "    #                    'zeros','ones','constant','RandomNormal','RandomUniform',\n",
    "    #                  'TruncatedNormal','VarianceScaling','orthogonal','identity',\n",
    "    #\n",
    "    #                  'he_uniform','he_normal']\n",
    "    #recurrent_initializer=['zeros',\n",
    "    #                       'ones','constant','RandomNormal','RandomUniform',\n",
    "    #                  'TruncatedNormal','VarianceScaling','orthogonal','identity',\n",
    "    #                  'lecun_uniform','lecun_normal','glorot_uniform','glorot_normal',\n",
    "    #                  'he_uniform','he_normal']\n",
    "    bias_initializer=['zeros','ones','glorot_normal','he_normal']\n",
    "    #                   'ones','constant','RandomNormal','RandomUniform',\n",
    "    #                  'TruncatedNormal','VarianceScaling','orthogonal','identity',\n",
    "    #                  'lecun_uniform','lecun_normal','glorot_uniform','glorot_normal',\n",
    "    #                  'he_uniform','he_normal']\n",
    "    kernel_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    recurrent_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    bias_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    activity_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    kernel_constraint=[None, 'MaxNorm']#'MinMaxNorm','NonNeg','UnitNorm',\n",
    "    recurrent_constraint=[None, 'MaxNorm']#'MinMaxNorm','NonNeg','UnitNorm',\n",
    "    bias_constraint=[None, 'MaxNorm']#'MinMaxNorm','NonNeg','UnitNorm',\n",
    "    dropout=[0.0, 0.2,0.3,0.4,0.5]\n",
    "    recurrent_dropout=[0.0, 0.2,0.3,0.4,0.5]\n",
    "    optimizer = ['Adadelta','Adam','Adamax','Nadam'] \n",
    "    \n",
    "    param_grid = dict(word_encoding_dim = word_encoding_dim,\n",
    "                      sentence_encoding_dim = sentence_encoding_dim,\n",
    "                      optimizer = optimizer,\n",
    "                      #activation = activation,\n",
    "                      #recurrent_activation = recurrent_activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                      #recurrent_initializer =  recurrent_initializer,\n",
    "                      bias_initializer = bias_initializer,\n",
    "                      kernel_regularizer = kernel_regularizer,\n",
    "                      recurrent_regularizer = recurrent_regularizer,\n",
    "                      bias_regularizer = bias_regularizer,\n",
    "                      activity_regularizer = activity_regularizer,\n",
    "                      kernel_constraint = kernel_constraint,\n",
    "                      recurrent_constraint = recurrent_constraint,\n",
    "                      bias_constraint = bias_constraint,\n",
    "                      dropout = dropout,\n",
    "                      recurrent_dropout = recurrent_dropout,\n",
    "                      kernel_initializer2 = kernel_initializer,\n",
    "                      bias_initializer2 = bias_initializer,\n",
    "                      kernel_regularizer2 = kernel_regularizer,\n",
    "                      recurrent_regularizer2 = recurrent_regularizer,\n",
    "                      bias_regularizer2 = bias_regularizer,\n",
    "                      activity_regularizer2 = activity_regularizer,\n",
    "                      kernel_constraint2 = kernel_constraint,\n",
    "                      recurrent_constraint2 = recurrent_constraint,\n",
    "                      bias_constraint2 = bias_constraint,\n",
    "                      dropout2 = dropout,\n",
    "                      recurrent_dropout2 = recurrent_dropout,\n",
    "                      batch_size = batch_size,\n",
    "                      epochs = epochs\n",
    "                      )\n",
    "    \n",
    "    #Evaluation metrics\n",
    "    scoring = {'acc':make_scorer(accuracy_score),'f1': make_scorer(f1_score),'f2': make_scorer(fbeta_score, beta=2),\n",
    "               'rec': make_scorer(recall_score)} \n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model,verbose=1 )\n",
    "    \n",
    "    grid = RandomizedSearchCV(cv=2,n_iter=2, #cv should be 4, n_iter 20\n",
    "                              estimator=model, \n",
    "                              param_distributions=param_grid,\n",
    "                              n_jobs=-1,\n",
    "                              scoring=scoring,\n",
    "                              refit='acc', #or f1, f2\n",
    "                              return_train_score = True\n",
    "                              #random_state = 42\n",
    "                              )\n",
    "    \n",
    "    \n",
    "    \"\"\"#fix\n",
    "    trainingDim = 500*3/4 \n",
    "    lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-1, \n",
    "                                 steps_per_epoch=np.ceil(trainingDim/batch_size[1]), \n",
    "                                 epochs=3)\n",
    "    \"\"\"\n",
    "\n",
    "    lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)    \n",
    "                       \n",
    "    grid_result = grid.fit(X, Y, callbacks=[lr_sched])    \n",
    "    \n",
    "    \"\"\"   \n",
    "    lr_finder.plot_loss('lr_loss.png')\n",
    "    lr_finder.plot_lr('lr.png')\n",
    "    \"\"\"\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "    mean_acc = grid_result.cv_results_['mean_test_acc']\n",
    "    mean_f1 = grid_result.cv_results_['mean_test_f1']\n",
    "    mean_f2 = grid_result.cv_results_['mean_test_f2']\n",
    "    mean_rec = grid_result.cv_results_['mean_test_rec']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    #look into grid_result_v: rank_test_rec, rank_test_f2 give a ranking of the models for both parameters++\n",
    "    \n",
    "    for mean0, mean1, mean2, mean3, param in zip(mean_acc, mean_f1, mean_f2, mean_rec, params):\n",
    "        f.write(\"acc %f f1 %f f2 %f rec %f with: %r\\n\" % (mean0, mean1, mean2, mean3, param))\n",
    "    f.write('---------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(grid.cv_results_)\n",
    "res.to_csv('HAN_params.csv')\n",
    "\n",
    "#Test here\n",
    "y_pred = grid.predict(x_test)\n",
    "f.write(\"The final accuracy is: \")\n",
    "somme = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]==y_pred[i]:\n",
    "        somme+=1\n",
    "print(somme,len(y_test))\n",
    "avg = somme/len(y_test)\n",
    "f.write(\"%f\"%avg)\n",
    "f.close()\n",
    "\n",
    "print('y_pred:',y_pred)\n",
    "print('y_test',y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
