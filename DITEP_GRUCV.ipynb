{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Flatten\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding, Flatten\n",
    "from keras.optimizers import RMSprop,Adagrad,Adadelta,Adam,Adamax,Nadam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model, Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer,f1_score,recall_score,fbeta_score,precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=10):\n",
    "        '''\n",
    "        Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "        '''\n",
    "        def schedule(epoch):\n",
    "            return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "        \n",
    "        return LearningRateScheduler(schedule)\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25): #alternative loss function to try\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM/GRU search\")\n",
    "#Processing the labels of the raw IMDB data\n",
    "imdb_dir = ''\n",
    "train_dir = os.path.join(imdb_dir, 'X_REP_RAW')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding='utf-8',errors='ignore')\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_samples = len(labels)\n",
    "maxlen = 1000 #cutoff reviews, put 2000\n",
    "training_samples = int(0.8 * total_samples )\n",
    "test_samples = total_samples - training_samples\n",
    "max_words = 10000 #29107 is the total number of words, 27610 is w2v vocabulary dimension, put at least 10000 \n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen) #Pads sequences to the same length.\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "X = data[:training_samples]\n",
    "Y = labels[:training_samples]\n",
    "x_test = data[training_samples: training_samples + test_samples]\n",
    "y_test = labels[training_samples: training_samples + test_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parsing the GloVe word-embeddings file\n",
    "w2v_dir = ''\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(w2v_dir, 'w2v_reports_256.vec'), encoding='utf-8',errors='ignore') #even 300\n",
    "dummy = f.readline() #to skip the first line that tells: n.of words n.of features\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#OSS: THIS CAN BE REMOVED IF I DON'T WANT PRETRAINED WORD EMBEDDINGS####\n",
    "#Preparing the GloVe word-embeddings matrix\n",
    "embedding_dim = 256 #number of features, increaseable to 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(optimizer,\n",
    "                 kernel_initializer,\n",
    "                     bias_initializer,\n",
    "                      kernel_regularizer,\n",
    "                      recurrent_regularizer,\n",
    "                      bias_regularizer,\n",
    "                      activity_regularizer,\n",
    "                      kernel_constraint,\n",
    "                      recurrent_constraint,\n",
    "                      bias_constraint,\n",
    "                      dropout,\n",
    "                      recurrent_dropout,loss):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    for i in range(len(layer_list)):\n",
    "        if rnntype=='LSTM':\n",
    "            model.add(layers.Bidirectional(layers.LSTM(layer_list[i], \n",
    "                                                       #activation = activation,\n",
    "                     # recurrent_activation = recurrent_activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                     # recurrent_initializer =  recurrent_initializer,\n",
    "                      bias_initializer = bias_initializer,\n",
    "                      kernel_regularizer = kernel_regularizer,\n",
    "                      recurrent_regularizer = recurrent_regularizer,\n",
    "                      bias_regularizer = bias_regularizer,\n",
    "                      activity_regularizer = activity_regularizer,\n",
    "                      kernel_constraint = kernel_constraint,\n",
    "                      recurrent_constraint = recurrent_constraint,\n",
    "                      bias_constraint = bias_constraint,\n",
    "                      dropout = dropout,\n",
    "                      recurrent_dropout = recurrent_dropout,\n",
    "                                                       return_sequences = True)) )\n",
    "        else:\n",
    "            model.add(layers.Bidirectional(layers.GRU(layer_list[i],\n",
    "                     #                                 activation = activation,\n",
    "                      #recurrent_activation = recurrent_activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                     # recurrent_initializer =  recurrent_initializer,\n",
    "                      bias_initializer = bias_initializer,\n",
    "                      kernel_regularizer = kernel_regularizer,\n",
    "                      recurrent_regularizer = recurrent_regularizer,\n",
    "                      bias_regularizer = bias_regularizer,\n",
    "                      activity_regularizer = activity_regularizer,\n",
    "                      kernel_constraint = kernel_constraint,\n",
    "                      recurrent_constraint = recurrent_constraint,\n",
    "                      bias_constraint = bias_constraint,\n",
    "                      dropout = dropout,\n",
    "                      recurrent_dropout = recurrent_dropout,\n",
    "                                                      return_sequences = True)) )\n",
    "\n",
    "    if rnntype=='LSTM':\n",
    "        model.add(layers.Bidirectional(layers.LSTM(layer_list[-1], \n",
    "                                                #   activation = activation,\n",
    "                     # recurrent_activation = recurrent_activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                     # recurrent_initializer =  recurrent_initializer,\n",
    "                      bias_initializer = bias_initializer,\n",
    "                      kernel_regularizer = kernel_regularizer,\n",
    "                      recurrent_regularizer = recurrent_regularizer,\n",
    "                      bias_regularizer = bias_regularizer,\n",
    "                      activity_regularizer = activity_regularizer,\n",
    "                      kernel_constraint = kernel_constraint,\n",
    "                      recurrent_constraint = recurrent_constraint,\n",
    "                      bias_constraint = bias_constraint,\n",
    "                      dropout = dropout,\n",
    "                      recurrent_dropout = recurrent_dropout,)) )\n",
    "    else:\n",
    "        model.add(layers.Bidirectional(layers.GRU(layer_list[-1],\n",
    "                                  #  activation = activation,\n",
    "                      #recurrent_activation = recurrent_activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                     # recurrent_initializer =  recurrent_initializer,\n",
    "                      bias_initializer = bias_initializer,\n",
    "                      kernel_regularizer = kernel_regularizer,\n",
    "                      recurrent_regularizer = recurrent_regularizer,\n",
    "                      bias_regularizer = bias_regularizer,\n",
    "                      activity_regularizer = activity_regularizer,\n",
    "                      kernel_constraint = kernel_constraint,\n",
    "                      recurrent_constraint = recurrent_constraint,\n",
    "                      bias_constraint = bias_constraint,\n",
    "                      dropout = dropout,\n",
    "                      recurrent_dropout = recurrent_dropout)) )\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    #OSS: THIS CAN BE REMOVED IF I DON'T WANT PRETRAINED WORD EMBEDDINGS####\n",
    "    #Loading pretrained word embeddings into the Embedding layer\n",
    "    model.layers[0].set_weights([embedding_matrix]) \n",
    "    model.layers[0].trainable = False\n",
    "    \n",
    "    model.compile(optimizer=optimizer,loss=loss)#,metrics=['acc',mean_pred])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single cross validation (i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('GRU-LSTM-gridoutput.txt','w')\n",
    "for i in range(1):\n",
    "    \n",
    "    num_layers = np.random.randint(1,5)\n",
    "    layers_dim = [16,32,64,128] #units\n",
    "    layer_list = sorted(random.sample(layers_dim,num_layers), reverse=True)\n",
    "    rnntype = random.sample(['LSTM','GRU'],1)[0]\n",
    "    epoch_size = [50]# random.sample(list(range(6,16,2)),1)[0]\n",
    "    batch_size = random.sample([16,32,64,128],1)[0]\n",
    "    \n",
    "    #GRU params\n",
    "    #activation = ['hard_sigmoid','softmax','elu','selu','softplus','softsign','relu',\n",
    "    #                      'tanh','sigmoid','exponential','linear','PReLU','LeakyReLu']\n",
    "    #recurrent_activation=['hard_sigmoid','softmax','elu','selu','softplus','softsign','relu',\n",
    "    #                      'tanh','sigmoid','exponential','linear','PReLU','LeakyReLu']\n",
    "    kernel_initializer=['glorot_normal','glorot_uniform','TruncatedNormal','VarianceScaling'] #cause it's a tanh\n",
    "    #                    'zeros','ones','constant','RandomNormal','RandomUniform',\n",
    "    #                  'TruncatedNormal','VarianceScaling','orthogonal','identity',\n",
    "    #\n",
    "    #                  'he_uniform','he_normal']\n",
    "    #recurrent_initializer=['zeros',\n",
    "    #                       'ones','constant','RandomNormal','RandomUniform',\n",
    "    #                  'TruncatedNormal','VarianceScaling','orthogonal','identity',\n",
    "    #                  'lecun_uniform','lecun_normal','glorot_uniform','glorot_normal',\n",
    "    #                  'he_uniform','he_normal']\n",
    "    bias_initializer=['zeros','ones','glorot_normal','he_normal']\n",
    "    #                   'ones','constant','RandomNormal','RandomUniform',\n",
    "    #                  'TruncatedNormal','VarianceScaling','orthogonal','identity',\n",
    "    #                  'lecun_uniform','lecun_normal','glorot_uniform','glorot_normal',\n",
    "    #                  'he_uniform','he_normal']\n",
    "    kernel_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    recurrent_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    bias_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    activity_regularizer=[None, 'l1','l2','l1_l2']\n",
    "    kernel_constraint=[None, 'MaxNorm']#'MinMaxNorm','NonNeg','UnitNorm',\n",
    "    recurrent_constraint=[None, 'MaxNorm']#'MinMaxNorm','NonNeg','UnitNorm',\n",
    "    bias_constraint=[None, 'MaxNorm']#'MinMaxNorm','NonNeg','UnitNorm',\n",
    "    dropout=[0.0, 0.2,0.3,0.4,0.5]\n",
    "    recurrent_dropout=[0.0, 0.2,0.3,0.4,0.5]\n",
    "    \n",
    "    optimizer = ['Adadelta','Adam','Adamax','Nadam']\n",
    "    loss = ['binary_crossentropy']#,[focal_loss()]]\n",
    "    \n",
    "    \n",
    "    param_grid = dict(optimizer = optimizer,\n",
    "                      #activation = activation,\n",
    "                      #recurrent_activation = recurrent_activation,\n",
    "                      kernel_initializer = kernel_initializer,\n",
    "                      #recurrent_initializer =  recurrent_initializer,\n",
    "                      bias_initializer = bias_initializer,\n",
    "                      kernel_regularizer = kernel_regularizer,\n",
    "                      recurrent_regularizer = recurrent_regularizer,\n",
    "                      bias_regularizer = bias_regularizer,\n",
    "                      activity_regularizer = activity_regularizer,\n",
    "                      kernel_constraint = kernel_constraint,\n",
    "                      recurrent_constraint = recurrent_constraint,\n",
    "                      bias_constraint = bias_constraint,\n",
    "                      dropout = dropout,\n",
    "                      recurrent_dropout = recurrent_dropout,\n",
    "                      loss = loss\n",
    "                      )\n",
    "    \n",
    "    scoring = {'acc':make_scorer(accuracy_score),'f1': make_scorer(f1_score),'f2': make_scorer(fbeta_score, beta=2),\n",
    "               'rec': make_scorer(recall_score)} \n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_model,verbose=0)\n",
    "    \n",
    "    grid = RandomizedSearchCV(cv=2, #4\n",
    "                              n_iter=2, #10\n",
    "                              estimator=model, \n",
    "                              param_distributions=param_grid,\n",
    "                              n_jobs=-1, \n",
    "                              scoring=scoring, \n",
    "                              refit='acc', #or f1, f2\n",
    "                              #random_state = 42\n",
    "                             )\n",
    "    \n",
    "    \"\"\"#fix\n",
    "    lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "    \"\"\"                              \n",
    "    grid_result = grid.fit(X, Y)#, callbacks=[lr_finder])\n",
    "    \n",
    "    \"\"\"    \n",
    "    lr_finder.plot_loss('lr_loss.png')\n",
    "    lr_finder.plot_lr('lr.png')\n",
    "    \"\"\"\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    \n",
    "    #mean_acc = grid_result.cv_results_['mean_test_acc']\n",
    "    mean_f1 = grid_result.cv_results_['mean_test_f1']\n",
    "    mean_f2 = grid_result.cv_results_['mean_test_f2']\n",
    "    mean_rec = grid_result.cv_results_['mean_test_rec']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    #look into grid_result_v: rank_test_rec, rank_test_f2 give a ranking of the models for both parameters++\n",
    "    \n",
    "    f.write('rnn type: %s\\n'%rnntype)\n",
    "    for item in layer_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    f.write('epochs: %d\\n'%epoch_size)\n",
    "    f.write('batch_dim: %d\\n'%batch_size)\n",
    "    for mean1, mean2, mean3, param in zip(mean_f1, mean_f2, mean_rec, params):\n",
    "        f.write(\"f1 %f f2 %f rec %f with: %r\" % (mean1, mean2, mean3, param))\n",
    "    f.write('---------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(grid.cv_results_)\n",
    "res.to_csv('GRU_params.csv')\n",
    "\n",
    "#Test here\n",
    "y_pred = grid.best_estimator_.predict(x_test)\n",
    "f.write(\"The final accuracy is: \")\n",
    "somme = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]==y_pred[i]:\n",
    "        somme+=1\n",
    "print(somme,len(y_test))\n",
    "avg = somme/len(y_test)\n",
    "f.write(\"%f\"%avg)\n",
    "f.close()\n",
    "\n",
    "print('y_pred:',y_pred)\n",
    "print('y_test',y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
