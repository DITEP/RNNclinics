{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate,Flatten, Activation, Add\n",
    "from keras.layers import Convolution1D, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import Embedding, ThresholdedReLU\n",
    "from keras.layers import AlphaDropout, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Class to handle loading and processing of raw datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\",\n",
    "                 input_size, num_of_classes=2):\n",
    "        \"\"\"\n",
    "        Initialization of a Data object.\n",
    "        Args:\n",
    "            data_source (str): Raw data file path\n",
    "            alphabet (str): Alphabet of characters to index\n",
    "            input_size (int): Size of input features\n",
    "            num_of_classes (int): Number of classes in data\n",
    "        \"\"\"\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_size = len(self.alphabet)\n",
    "        self.dict = {}  # Maps each character to an integer\n",
    "        self.no_of_classes = num_of_classes\n",
    "        for idx, char in enumerate(self.alphabet):\n",
    "            self.dict[char] = idx + 1\n",
    "        self.length = input_size\n",
    "       \n",
    "\n",
    "    def load_data(self,samples_number,randoms,subfolder='train'):\n",
    "        \"\"\"\n",
    "        Load raw data from the source file into data variable.\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        ditep_dir = 'aclImdb'\n",
    "        ditep_dir = os.path.join(ditep_dir, subfolder)\n",
    "        labels = []\n",
    "        texts = []\n",
    "        data = []\n",
    "        for label_type in ['neg', 'pos']:\n",
    "            dir_name = os.path.join(ditep_dir, label_type)\n",
    "            for fname in sorted(os.listdir(dir_name)):\n",
    "                if fname[-4:] == '.txt':\n",
    "                    f = open(os.path.join(dir_name, fname),encoding='utf-8',errors='ignore')\n",
    "                    texts.append(f.read())\n",
    "                    f.close()\n",
    "                    if label_type == 'neg':\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(1)\n",
    "        \n",
    "        for i in range(samples_number):\n",
    "            random_num=randoms[i]\n",
    "            data.append((labels[random_num], texts[random_num]))  # format: (label, text)\n",
    "        self.data = np.array(data)\n",
    "        \n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Return all loaded data from data variable.\n",
    "        Returns:\n",
    "            (np.ndarray) Data transformed from raw to indexed form with associated one-hot label.\n",
    "        \"\"\"\n",
    "        data_size = len(self.data)\n",
    "        start_index = 0\n",
    "        end_index = data_size\n",
    "        batch_texts = self.data[start_index:end_index]\n",
    "        batch_indices = []\n",
    "        #one_hot = np.eye(self.no_of_classes, dtype='int64')\n",
    "        classes = []\n",
    "        for c, s in batch_texts:\n",
    "            batch_indices.append(self.str_to_indexes(s))        \n",
    "            classes.append(int(c))#one_hot[int(c)])\n",
    "        return np.asarray(batch_indices, dtype='int64'), np.asarray(classes)\n",
    "\n",
    "    def str_to_indexes(self, s):\n",
    "        \"\"\"\n",
    "        Convert a string to character indexes based on character dictionary.\n",
    "        \n",
    "        Args:\n",
    "            s (str): String to be converted to indexes\n",
    "        Returns:\n",
    "            str2idx (np.ndarray): Indexes of characters in s\n",
    "        \"\"\"\n",
    "        s = s.lower()\n",
    "        max_length = min(len(s), self.length)\n",
    "        str2idx = np.zeros(self.length, dtype='int64')\n",
    "        for i in range(1, max_length + 1):\n",
    "            c = s[-i]\n",
    "            if c in self.dict:\n",
    "                str2idx[i - 1] = self.dict[c]\n",
    "        return str2idx\n",
    "\n",
    "\n",
    "class CharCNNKim(object):\n",
    "    \"\"\"\n",
    "    Class to implement the Character Level Convolutional Neural Network\n",
    "    as described in Kim et al., 2015 (https://arxiv.org/abs/1508.06615)\n",
    "    Their model has been adapted to perform text classification instead of language modelling\n",
    "    by replacing subsequent recurrent layers with dense layer(s) to perform softmax over classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, alphabet_size, embedding_size,\n",
    "                 conv_layers, fully_connected_layers,\n",
    "                 num_of_classes, dropout_p,\n",
    "                 optimizer='adam', loss='categorical_crossentropy'):\n",
    "        \"\"\"\n",
    "        Initialization for the Character Level CNN model.\n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            alphabet_size (int): Size of alphabets to create embeddings for\n",
    "            embedding_size (int): Size of embeddings\n",
    "            conv_layers (list[list[int]]): List of Convolution layers for model\n",
    "            fully_connected_layers (list[list[int]]): List of Fully Connected layers for model\n",
    "            num_of_classes (int): Number of classes in data\n",
    "            dropout_p (float): Dropout Probability\n",
    "            optimizer (str): Training optimizer\n",
    "            loss (str): Loss function\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fully_connected_layers = fully_connected_layers\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.dropout_p = dropout_p\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self._build_model()  # builds self.model variable\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile the Character Level CNN model\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        inputs = Input(shape=(self.input_size,), name='sent_input', dtype='int64')\n",
    "        # Embedding layers\n",
    "        x = Embedding(self.alphabet_size + 1, self.embedding_size, input_length=self.input_size)(inputs)\n",
    "        # Convolution layers\n",
    "        convolution_output = []\n",
    "        for num_filters, filter_width,dummy in self.conv_layers:\n",
    "            conv = Convolution1D(filters=num_filters,\n",
    "                                 kernel_size=filter_width,\n",
    "                                 activation='relu',\n",
    "                                 name='Conv1D_{}_{}'.format(num_filters, filter_width))(x)\n",
    "            pool = GlobalMaxPooling1D(name='MaxPoolingOverTime_{}_{}'.format(num_filters, filter_width))(conv)\n",
    "            convolution_output.append(pool)\n",
    "        x = Concatenate()(convolution_output)\n",
    "        # Fully connected layers\n",
    "        for fl in self.fully_connected_layers:\n",
    "            x = Dense(fl, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "            x = AlphaDropout(self.dropout_p)(x)\n",
    "        # Output layer\n",
    "        predictions = Dense(self.num_of_classes, activation='sigmoid')(x)\n",
    "        # Build and compile model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[recall])\n",
    "        self.model = model\n",
    "        print(\"CharCNNKim model built: \")\n",
    "        self.model.summary()\n",
    "\n",
    "    def train(self, training_inputs, training_labels,\n",
    "              validation_inputs, validation_labels,\n",
    "              epochs, batch_size, checkpoint_every=100):\n",
    "        \"\"\"\n",
    "        Training function\n",
    "        Args:\n",
    "            training_inputs (numpy.ndarray): Training set inputs\n",
    "            training_labels (numpy.ndarray): Training set labels\n",
    "            validation_inputs (numpy.ndarray): Validation set inputs\n",
    "            validation_labels (numpy.ndarray): Validation set labels\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size\n",
    "            checkpoint_every (int): Interval for logging to Tensorboard\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Start training\n",
    "        print(\"Training CharCNNKim model: \")\n",
    "        history = self.model.fit(training_inputs, training_labels,\n",
    "                       validation_data=(validation_inputs, validation_labels),\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=2)\n",
    "        return history\n",
    "\n",
    "    def test(self, testing_inputs, testing_labels, batch_size):\n",
    "        \"\"\"\n",
    "        Testing function\n",
    "        Args:\n",
    "            testing_inputs (numpy.ndarray): Testing set inputs\n",
    "            testing_labels (numpy.ndarray): Testing set labels\n",
    "            batch_size (int): Batch size\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Evaluate inputs\n",
    "        return  self.model.evaluate(testing_inputs, testing_labels, batch_size=batch_size, verbose=1)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "total_samples = 25000\n",
    "training_samples = 20000\n",
    "val = 5000\n",
    "input_size = 2600   #count the characters in a medical records\n",
    "alphabet_size = 69\n",
    "embedding_size = 300\n",
    "conv_layers = [[32,7,3],[32,5,-1]] #filters,kernel_size,maxpooling for every conv1d (obs:they have to be different)\n",
    "fully_connected_layers = [10] #dense units for every dense\n",
    "num_of_classes = 1\n",
    "dropout_p = 0.2\n",
    "optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "loss = 'binary_crossentropy'\n",
    "epochs = 15 \n",
    "batch_size = 16\n",
    "threshold = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = Data(input_size=input_size)\n",
    "randoms=list(map(int,(25000*np.random.rand(training_samples+val,1))))\n",
    "training_data.load_data(training_samples,randoms[:training_samples])\n",
    "x_train, y_train = training_data.get_all_data()\n",
    "# Load validation data\n",
    "validation_data = Data(input_size=input_size)\n",
    "validation_data.load_data(val,randoms[training_samples:training_samples+val])\n",
    "x_val, y_val = validation_data.get_all_data()\n",
    "\n",
    "charcnnkim = CharCNNKim(input_size,alphabet_size,embedding_size,conv_layers,fully_connected_layers,\n",
    "           num_of_classes,dropout_p,optimizer,loss)\n",
    "\n",
    "history = charcnnkim.train(x_train,y_train,x_val,y_val,epochs,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec = history.history['recall']\n",
    "val_rec = history.history['val_recall']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, rec, 'bo', label='Training rec')\n",
    "plt.plot(epochs, val_rec, 'b', label='Validation rec')\n",
    "plt.title('Training and validation rec')\n",
    "plt.savefig('DITEP_charConv_rec.png')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('DITEP_charConv_loss.png')\n",
    "\n",
    "test_data = Data(input_size=input_size)\n",
    "randoms=list(map(int,(25000*np.random.rand(val,1))))\n",
    "test_data.load_data(val,randoms,'test')\n",
    "x_test, y_test = test_data.get_all_data()\n",
    "\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "#model.load_weights('pre_trained_glove_model.h5')\n",
    "scores = charcnnkim.test(x_test, y_test,batch_size) \n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "scores.extend(['val_acc:'])\n",
    "scores.extend(val_acc)\n",
    "f=open('out_charconv.txt','w')\n",
    "temp=''\n",
    "for i in scores:\n",
    "    temp+=str(i)\n",
    "    temp+='\\n'\n",
    "f.write(temp)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
