# RNNclinics
Dans le code RNN+LSTM+word2vec on trouve un modèle qui entraine une LSTM (les détails modèle du réseau utilisé ici ne sont pas importants) en utilisant un word embedding issu d’une très grande base de données (plus que 3 milliards de mots) pre-entrainé par Google. Néanmoins, les résultats obtenus après l’entrainement du modèle sont très médiocres... Ceci est aussi cause du fait que la LSTM a seulement 16 unités, les epochs sont seulement 6, les séquences sont coupées à 30 mots. Tout cela peut être amélioré.


 
